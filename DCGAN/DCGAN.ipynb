{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9685a9-1613-4232-9869-e7296cdcabe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing considering the mid 77th slice\n",
    "\n",
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, image_paths, transforms=None) -> None:\n",
    "        # Filter out non-existent or empty files\n",
    "        valid_paths = []\n",
    "        for path in image_paths:\n",
    "            if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "                valid_paths.append(path)\n",
    "            else:\n",
    "                print(f\"[Warning] Skipping invalid or missing file: {path}\")\n",
    "\n",
    "        if len(valid_paths) == 0:\n",
    "            raise ValueError(\"No valid image files found!\")\n",
    "\n",
    "        self.imagePaths = valid_paths\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagePaths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imagePath = self.imagePaths[index]\n",
    "        try:\n",
    "            nii_image = nib.load(imagePath)\n",
    "            image = nii_image.get_fdata()[:, :, 77]\n",
    "            image = np.uint8(image / image.max() * 255)\n",
    "            image = Image.fromarray(image)\n",
    "\n",
    "            if self.transforms is not None:\n",
    "                image = self.transforms(image)\n",
    "\n",
    "            return image\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to load or process: {imagePath}\\n{e}\")\n",
    "            # Optionally: Return a blank image or raise the error\n",
    "            raise e\n",
    "\n",
    "    def save(self, store_path):\n",
    "        os.makedirs(store_path, exist_ok=True)\n",
    "\n",
    "        for i, impath in enumerate(self.imagePaths):\n",
    "            try:\n",
    "                nii_image = nib.load(impath)\n",
    "                image = nii_image.get_fdata()[:, :, 77]\n",
    "                image = np.uint8(image / image.max() * 255)\n",
    "                image = Image.fromarray(image)\n",
    "\n",
    "                if self.transforms is not None:\n",
    "                    image = self.transforms(image)\n",
    "\n",
    "                vutils.save_image(image, f'{store_path}/{i}.png')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[Warning] Skipped saving image {i} ({impath}) due to error:\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de86bae-7d62-420c-871b-12a1c17697f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data import BraTSDataset\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fea59-95f5-4b83-87b7-57810c40c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psnr(real_batch: np.ndarray, fake_batch: np.ndarray) -> float:\n",
    "    b_size = real_batch.shape[0]\n",
    "    psnr_val = 0.0\n",
    "    for i in range(b_size):\n",
    "        psnr_val += psnr(\n",
    "            real_batch[i, :, :, :].transpose(1, 2, 0),\n",
    "            fake_batch[i, :, :, :].transpose(1, 2, 0),\n",
    "            data_range=1.0,\n",
    "        )\n",
    "    return psnr_val / b_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a82cb36-58ed-43dc-b33f-782650129a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ssim(real_batch: np.ndarray, fake_batch: np.ndarray) -> float:\n",
    "    b_size = real_batch.shape[0]\n",
    "    ssim_val = 0.0\n",
    "    for i in range(b_size):\n",
    "        ssim_val += ssim(\n",
    "            real_batch[i, :, :, :],\n",
    "            fake_batch[i, :, :, :],\n",
    "            channel_axis=0,\n",
    "            data_range=1.0,\n",
    "        )\n",
    "    return ssim_val / b_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965e9b4-1b01-42b9-a919-e060a8d9e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config for training\n",
    "dataset_root = \"dataset\"\n",
    "t1_train_data = \"T1c_BraTS_2023\"\n",
    "image_size = 64\n",
    "batch_size = 8\n",
    "#batch_size = 128\n",
    "num_workers = 16\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Generator model configuration\n",
    "latent_size = 128\n",
    "feature_map_size = image_size//2 # was 32 before\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.0002\n",
    "# beta 1 for Adam\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150a77c-54f8-492c-9efd-74fd8b3af45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_paths = [os.path.join(t1_train_data, impath) for impath in os.listdir(t1_train_data)]\n",
    "image_paths = [\n",
    "    os.path.join(t1_train_data, f)\n",
    "    for f in os.listdir(t1_train_data)\n",
    "    if (f.endswith(\".nii\") or f.endswith(\".nii.gz\")) and not f.startswith(\"._\")\n",
    "]\n",
    "\n",
    "tf = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "dataset = BraTSDataset(image_paths, tf)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5758163-79d6-4cc5-a5bf-3dce565d8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    \"\"\"Custom weight initialization called on netG and netD\"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.normal_(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab114cab-2cb7-497b-b7c2-fc765034b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator code\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input: latent_size x 1 x 1\n",
    "            nn.ConvTranspose2d(latent_size, feature_map_size * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(feature_map_size * 8),\n",
    "            nn.ReLU(True),\n",
    "            # input: feature_map_size*16 x image_size/16 x image_size/16\n",
    "            nn.ConvTranspose2d(feature_map_size * 8, feature_map_size * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_map_size * 4),\n",
    "            nn.ReLU(True),\n",
    "            # input: feature_map_size*8 x image_size/8 x image_size/8\n",
    "            nn.ConvTranspose2d(feature_map_size * 4, feature_map_size * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_map_size * 2),\n",
    "            nn.ReLU(True),\n",
    "            # input: feature_map_size*4 x image_size/4 x image_size/4\n",
    "            nn.ConvTranspose2d(feature_map_size*2, feature_map_size, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_map_size),\n",
    "            nn.ReLU(True),\n",
    "            # input: feature_map_size*2 x image_size/2 x image_size/2\n",
    "            nn.ConvTranspose2d(feature_map_size, 1, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "            # output: 1 x image_size x image_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf2d79-2c2b-4765-b84a-db075f8c038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator().to(device)\n",
    "netG.apply(weight_init)\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15b4e8-3014-4fad-8f60-49ad93220346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input: 1 x image_size x image_size\n",
    "            nn.Conv2d(1, feature_map_size, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_map_size),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # input: feature_map_size x image_size/2 x image_size/2\n",
    "            nn.Conv2d(feature_map_size, feature_map_size * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_map_size * 2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # input: feature_map_size*2 x image_size/4 x image_size/4\n",
    "            nn.Conv2d(feature_map_size * 2, feature_map_size * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_map_size * 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # input: feature_map_size*4 x image_size/8 x image_size/8\n",
    "            nn.Conv2d(feature_map_size * 4, feature_map_size * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_map_size * 8),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # input: feature_map_size*8 x image_size/16 x image_size/16\n",
    "            nn.Conv2d(feature_map_size * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # output: 1 x 1 x 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeaf133-fb73-45bb-a201-944182799b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = Discriminator().to(device)\n",
    "netD.apply(weight_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee951d92-7fcb-4e21-b670-e1493f77bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "critetion = nn.BCELoss()\n",
    "\n",
    "# fixed_noise = torch.randn(16, latent_size, 1, 1, device=device)\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13922e74-4ef4-4f98-a5b2-9a51b6c5449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "g_loss_hist = list()\n",
    "d_loss_hist = list()\n",
    "d_loss_real_hist = list()\n",
    "d_loss_fake_hist = list()\n",
    "psnr_hist = list()\n",
    "ssim_hist = list()\n",
    "\n",
    "best_psnr = -float(\"inf\")\n",
    "best_ssim = -float(\"inf\")\n",
    "\n",
    "best_g_weights = None\n",
    "best_d_weights = None\n",
    "\n",
    "print(\"Staring training loop...\")\n",
    "print(f\"Number of mini batch iterations per epoch: {len(dataloader)}\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in dataloader\n",
    "    for i, data in tqdm(enumerate(dataloader, 0)):\n",
    "        # Train with real batch\n",
    "        real = data.to(device)\n",
    "        b_size = real.size(0)\n",
    "\n",
    "        # Update D network\n",
    "        # maximize log(D(x)) using batch of real images\n",
    "        # maximize log(1 - D(G(z))) using batch of fake images\n",
    "        netD.zero_grad()\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        output = netD(real).view(-1)\n",
    "        errD_real = critetion(output, label)\n",
    "        errD_real.backward()\n",
    "\n",
    "        # Train with fake batch\n",
    "        noise = torch.randn(b_size, latent_size, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        errD_fake = critetion(output, label)\n",
    "        errD_fake.backward()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        d_loss_real_hist.append(errD_real.detach().cpu().numpy())\n",
    "        d_loss_fake_hist.append(errD_fake.detach().cpu().numpy())\n",
    "        d_loss_hist.append(errD.detach().cpu().numpy())\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Train generator: maximize log(D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake).view(-1)\n",
    "        errG = critetion(output, label)\n",
    "        errG.backward()\n",
    "\n",
    "        g_loss_hist.append(errG.detach().cpu().numpy())\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Save best model based on PSNR every iteration\n",
    "        psnr_hist.append(\n",
    "            compute_psnr(\n",
    "                real.detach().cpu().numpy(),\n",
    "                fake.detach().cpu().numpy()\n",
    "            )\n",
    "        )\n",
    "        ssim_hist.append(\n",
    "            compute_ssim(\n",
    "                real.detach().cpu().numpy(),\n",
    "                fake.detach().cpu().numpy()\n",
    "            )\n",
    "        )\n",
    "        if ssim_hist[-1] > best_ssim: #ssim from their code\n",
    "            best_psnr = psnr_hist[-1]\n",
    "            best_ssim = ssim_hist[-1]\n",
    "            best_g_weights = netG.state_dict()\n",
    "            best_d_weights = netD.state_dict()\n",
    "\n",
    "    # Summarize performance every epoch\n",
    "    tqdm.write(\"\\n\".join((\n",
    "        f\"epoch: {epoch}, Loss D: {d_loss_hist[-1]:.4f}, Loss G: {g_loss_hist[-1]:.4f} \",\n",
    "        f\"Loss D real: {d_loss_real_hist[-1]:.4f} \",\n",
    "        f\"Loss D fake: {d_loss_fake_hist[-1]:.4f} \",\n",
    "        f\"psnr: {psnr_hist[-1]:.4f}, best_psnr: {best_psnr:.4f} \",\n",
    "        f\"ssim: {ssim_hist[-1]:.4f}, best_ssim: {best_ssim:.4f} \",\n",
    "    )))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
